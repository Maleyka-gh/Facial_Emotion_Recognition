{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1yxddZdiD1XxCDoup1b6yQfVhXmIdD_Sd","timestamp":1680184589831}],"machine_shape":"hm","private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"nrqRU6lcDJmr"},"source":["import keras\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Dense, Flatten, Activation\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.optimizers import Adam\n","import pandas as pd\n","import numpy as np\n","import pydot_ng\n","import graphviz\n","import os\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib import pyplot\n","from matplotlib.pyplot import *\n","!pip install scikit-plot\n","import scikitplot as skplt\n","from sklearn.metrics import classification_report\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from keras.utils import np_utils"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_zRCf5AnpMe"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dc8iqiqD5_Ll"},"source":["\"\"\"\n","    num_classes = Defines the number of classes we have to predict which are namely \n","    Angry, Fear, Happy, Neutral, Surprise, Neutral and Disgust.\n","    \n","    From the exploratory Data Analysis we know that The Dimensions of the image are:\n","    Image Height = 48 pixels\n","    Image Width = 48 pixels\n","    Number of channels = 1 because it is a grayscale image.\n","    \n","    We will consider a batch size for the training of the image augmentation.\n","\n","\"\"\"\n","num_classes = 7\n","batch_size = 32\n","epochs = 100\n","img_height = 48\n","img_width = 48"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7eaBObw16_rt"},"source":["data = pd.read_csv('/content/drive/MyDrive/fer/fer2013.csv')\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M6E_NRtg7My6"},"source":["\"\"\"\n","    Reference: https://github.com/oarriaga/face_classification\n","    We will convert the pixels to list in this method. \n","    We split the data by spaces and then take them as arrays and \n","    reshape into 48, 48 shape. Then we expand the dimensions \n","    and then convert the labels to categorical matrix.\n","\n","\"\"\"\n","\n","pixels = data['pixels'].tolist() \n","faces = []\n","\n","for pixel_sequence in pixels:\n","    face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n","    face = np.asarray(face).reshape(img_height, img_width) \n","    faces.append(face.astype('float32'))\n","\n","faces = np.asarray(faces)\n","faces = np.expand_dims(faces, -1)\n","\n","emotions = pd.get_dummies(data['emotion']).values\n","len(emotions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emotions"],"metadata":{"id":"9sbAzAwSIYDF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# assume data is a pandas DataFrame with 'emotion' column containing emotion labels\n","emotion_labels = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}\n","\n","# create a figure and axis object\n","fig, ax = plt.subplots()\n","\n","# get the counts of each emotion label and sort by index\n","counts = data['emotion'].value_counts().sort_index()\n","\n","# define colors for each emotion label\n","colors = ['tab:red', 'tab:green', 'tab:orange', 'tab:blue', 'tab:purple', 'tab:brown', 'tab:gray']\n","\n","# create bar plot with counts and colors\n","ax.bar(emotion_labels.values(), counts, color=colors)\n","\n","# add labels and title\n","ax.set_xlabel('Emotion')\n","ax.set_ylabel('Count')\n","ax.set_title('Histogram of Emotion Labels')\n","\n","# add count labels above each bar\n","for i, v in enumerate(counts):\n","    ax.text(i, v+10, str(v), ha='center', fontsize=10)\n","\n","# rotate x-axis tick labels by 45 degrees\n","plt.xticks(rotation=45)\n","\n","# display the plot\n","plt.show()\n"],"metadata":{"id":"EAwJSssFC6Ej"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","#using stratify to ensure that class distribution is approximately the same in train and tets set\n","\n","X_train, X_test, y_train, y_test = train_test_split(faces, emotions, test_size=0.1, random_state=42,stratify=emotions)\n","X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=41 )"],"metadata":{"id":"KBopcQdQXtLM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = X_train / 255.\n","X_valid = X_valid / 255.\n","X_test = X_test / 255.\n"],"metadata":{"id":"egXzksEvPMI6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train.shape,y_train.shape, X_valid.shape,  y_valid.shape , X_test.shape , y_test.shape"],"metadata":{"id":"6tY4fKnCZc_q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Define the class labels\n","class_labels =emotion_labels\n","\n","\n","# Define the colors to use for each set\n","set_colors = ['blue', 'green', 'red']\n","\n","# Define the sets of labels to plot\n","label_sets = [y_train, y_valid, y_test]\n","set_names = ['Training Set', 'Validation Set', 'Test Set']\n","\n","# Get the counts for each set and class\n","counts = [np.sum(labels, axis=0) for labels in label_sets]\n","\n","# Create the subplot\n","fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n","\n","# Loop over the sets and plot the histograms\n","for i, ax in enumerate(axs.flat):\n","    # Create the histogram plot\n","    bars = ax.bar(class_labels.values(), counts[i], color=set_colors[i])\n","\n","    # Set the x-axis tick labels to be tilted by 45 degrees\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n","\n","    # Add the count number on top of each bar\n","    for bar in bars:\n","        height = bar.get_height()\n","        ax.annotate('{:.0f}'.format(height),\n","                    xy=(bar.get_x() + bar.get_width() / 2, height),\n","                    xytext=(0, 3),\n","                    textcoords=\"offset points\",\n","                    ha='center', va='bottom')\n","\n","    ax.set_xlabel('Emotion')\n","    ax.set_ylabel('Count')\n","    ax.set_title(set_names[i])\n","\n","# Set the overall title of the plot\n","fig.suptitle('Distribution of Emotions in Different Sets')\n","\n","# # Set the background color of the subplots to white\n","# plt.rcParams['axes.facecolor'] = 'white'\n","\n","# # Adjust the spacing between the subplots\n","# plt.subplots_adjust(wspace=0.3)\n","\n","# Show the plot\n","plt.show()\n"],"metadata":{"id":"8M-HKTnOzWfE"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tZQxPLPr8X7p"},"source":["def build_net(optim):\n","    img_input = Input(shape=(img_height, img_width, 1))\n","\n","    Conv1 = Conv2D(filters=32, kernel_size=(3,3), padding='same',\n","               kernel_initializer='he_normal', activation='relu')(img_input)\n","    B1 = BatchNormalization()(Conv1)\n","    M1 = MaxPooling2D(pool_size=(2,2))(B1)\n","    D1 = Dropout(0.5)(M1)\n","\n","    Conv2 = Conv2D(filters=64, kernel_size=(3,3), padding='same',\n","               kernel_initializer='he_normal', activation='relu')(D1)\n","    B2 = BatchNormalization()(Conv2)\n","    M2 = MaxPooling2D(pool_size=(2,2))(B2)\n","    D2 = Dropout(0.5)(M2)\n","\n","    Conv3 = Conv2D(filters=128, kernel_size=(3,3), padding='same',\n","               kernel_initializer='he_normal', activation='relu')(D2)\n","    B3 = BatchNormalization()(Conv3)\n","    M3 = MaxPooling2D(pool_size=(2,2))(B3)\n","    D3 = Dropout(0.5)(M3)\n","\n","    x = Flatten()(D3)\n","    DN1 = Dense(64, activation='relu', kernel_initializer='he_normal')(x)\n","    B4 = BatchNormalization()(DN1)\n","    D4 = Dropout(0.5)(B4)\n","\n","    output = Dense(num_classes, activation='softmax', kernel_initializer='he_normal')(D4)\n","\n","    model = Model(img_input, output)\n","\n","    model.compile(\n","        loss='categorical_crossentropy',\n","        optimizer=optim,\n","        metrics=['accuracy']\n","    )\n","\n","    model.summary()\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V1pvZVyt8X-j"},"source":["model = build_net(Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7))\n","model.summary()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict_x = model.predict(X_test)\n","classes_x = np.argmax(predict_x, axis=1)"],"metadata":{"id":"Fj8gJaX2hpd0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict_x"],"metadata":{"id":"r1z6ub9yiAdt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classes_x"],"metadata":{"id":"dgnJuF3nhulM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.utils import plot_model\n","plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"],"metadata":{"id":"ZYPR_9qf86Vg"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R6-He3Ze8YBI"},"source":["from tensorflow import keras\n","from keras.utils.vis_utils import plot_model\n","from keras.utils import np_utils\n","\n","keras.utils.plot_model(model, to_file='model1.png', show_layer_names=True,rankdir='LR',dpi=90)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rfn3L81a9MF4"},"source":["from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras.callbacks import TensorBoard\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","tensorboard = TensorBoard(log_dir='logs1')\n","lr_reducer = ReduceLROnPlateau(monitor='val_accuracy', factor=0.9, patience=3, verbose=1)\n","early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=8, verbose=1, mode='auto')\n","checkpointer = ModelCheckpoint(\"emotions1.h5\", monitor='val_accuracy', verbose=1, save_best_only=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bo-unNsn-U2e"},"source":["# As the data in hand is less as compared to the task so ImageDataGenerator is good to go.\n","train_datagen = ImageDataGenerator(\n","    rotation_range=15,\n","    width_shift_range=0.15,\n","    height_shift_range=0.15,\n","    shear_range=0.15,\n","    zoom_range=0.15,\n","    horizontal_flip=True,\n",")\n","train_datagen.fit(X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Plot a signle sample with its augmented versions**"],"metadata":{"id":"1SW7GVZuVTBD"}},{"cell_type":"code","source":["# # select one image from the training data\n","# sample_image = X_train[0]\n","\n","# # reshape the image to have a batch size of 1\n","# sample_image = sample_image.reshape((1,) + sample_image.shape)\n","# # sample_image.shape\n","\n","# # generate batches of augmented images\n","# i = 0\n","# for X_batch, y_batch in train_datagen.flow(sample_image,y_train[0:1], batch_size=1):\n","#     plt.figure(i)\n","#     plt.imshow(X_batch[0].reshape(img_width, img_height),cmap=('gray'))\n","#     i += 1\n","#     if i > 9:\n","#         break\n","\n","# plt.show()\n","# # # This will generate the augmented images using the parameters specified in the ImageDataGenerator and plot the original image as well as its augmented versions. The number of augmented images to plot is determined by the number of iterations in the for loop (in this case, it is 5). You can increase or decrease the number of iterations to generate more or fewer augmented images.\n","\n","\n","\n","\n"],"metadata":{"id":"bqwuRvPATWmc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # select one image from the training data\n","# sample_image = X_train[0]\n","\n","# # reshape the image to have a batch size of 1\n","# sample_image = sample_image.reshape((1,) + sample_image.shape)\n","\n","# fig, axs = plt.subplots(5, figsize=(5, 20))\n","\n","# # generate batches of augmented images\n","# for i, X_batch in enumerate(train_datagen.flow(sample_image, batch_size=1)):\n","#     if i >= 5:\n","#         break\n","#     axs[i].imshow(X_batch[0].reshape(img_width,img_height),cmap='gray')\n","\n","# plt.show()\n","\n","\n","\n"],"metadata":{"id":"PGxVrXKQVe2-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import matplotlib.pyplot as plt\n","\n","# # Pick a single image from the dataset\n","# image = X_train[6]\n","\n","# # Generate augmented images\n","# for X_batch, y_batch in train_datagen.flow(image.reshape(1,img_width, img_height,1),y_train[0:1],batch_size=1):\n","#     # Plot the original image\n","#     plt.subplot(121)\n","#     plt.imshow(image.reshape(img_width, img_height), cmap=('gray'))\n","#     plt.title('Original')\n","#     # Plot the augmented image\n","#     plt.subplot(122)\n","#     plt.imshow(X_batch[0].reshape(img_width, img_height), cmap=('gray'))\n","#     plt.title('Augmented')\n","#     # Show the plot\n","#     plt.show()\n","#     break"],"metadata":{"id":"E8_lmPygK0hK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # import matplotlib.pyplot as plt\n","\n","# # Generate a batch of augmented images\n","# for X_batch, y_batch in train_datagen.flow(X_train, y_train, batch_size=9):\n","#     # Create a grid of 3x3 images\n","#     plt.figure(figsize=(8,8))\n","#     for i in range(0, 9):\n","#         plt.subplot(330 + 1 + i)\n","#         plt.imshow(X_batch[i].reshape(img_width, img_height), cmap=plt.get_cmap('gray'))\n","#     # Show the plot\n","#     plt.show()\n","#     break\n","# # This code will show a grid of 3x3 augmented images, generated from the X_train dataset, and will break after one iteration.\n","# # Note that you will need to adjust the img_width, img_height, and channels variables to match the shape of your images.\n","\n","\n","\n","\n"],"metadata":{"id":"QgyYP15iHzkS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Augmentation + HP-Tune"],"metadata":{"id":"YzEvu08_200P"}},{"cell_type":"code","metadata":{"id":"n2oQZ7hs-YZd"},"source":["# #train with augmentation + hp tune\n","# batch_size = 32 #batch size of 32 performs the best.\n","# # epochs = 100\n","\n","\n","# history = model.fit_generator(\n","#     train_datagen.flow(X_train, y_train, batch_size=batch_size),\n","#     validation_data=(X_valid, y_valid),\n","#     steps_per_epoch=len(X_train) / batch_size,\n","#     epochs=epochs,\n","#     callbacks=[lr_reducer, tensorboard, early_stopper, checkpointer],\n","#     use_multiprocessing=True\n","# )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Augmentation"],"metadata":{"id":"rBG8Ml6I263E"}},{"cell_type":"code","source":["#train with augmentation\n","batch_size = 32 #batch size of 32 performs the best.\n","epochs = 100\n","\n","\n","history = model.fit_generator(\n","    train_datagen.flow(X_train, y_train, batch_size=batch_size),\n","    validation_data=(X_valid, y_valid),\n","    steps_per_epoch=len(X_train) / batch_size,\n","    epochs=epochs,\n","    callbacks=[tensorboard, early_stopper, checkpointer],\n","    use_multiprocessing=True\n",")"],"metadata":{"id":"m2tT3Rw125df"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Without Augmentation"],"metadata":{"id":"JDhyJIPb2h7x"}},{"cell_type":"code","metadata":{"id":"PBnyrP1L9Xsx"},"source":["# #train without augmentation \n","# history= model.fit(np.array(X_train), np.array(y_train),\n","#           batch_size=32,\n","#           steps_per_epoch=len(X_train) / batch_size,\n","#           epochs=epochs,\n","#           verbose=1,\n","#           validation_data=(np.array(X_valid), np.array(y_valid)),\n","#           shuffle=True,\n","#           callbacks=[tensorboard, early_stopper, checkpointer])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Without Augmentation + HP Tuning"],"metadata":{"id":"9dIQVdKi2cgX"}},{"cell_type":"code","source":["# #train without augmentation + HP Tuning\n","# history= model.fit(np.array(X_train), np.array(y_train),\n","#           batch_size=32,\n","#           steps_per_epoch=len(X_train) / batch_size,\n","#           epochs=epochs,\n","#           verbose=1,\n","#           validation_data=(np.array(X_valid), np.array(y_valid)),\n","#           shuffle=True,\n","#           callbacks=[lr_reducer, tensorboard, early_stopper, checkpointer])"],"metadata":{"id":"t7oAARPA2cJL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# history.epoch"],"metadata":{"id":"CPbVVZCi2bXm"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C2hr8wOyD4vc"},"source":["sns.set()\n","fig, axes = pyplot.subplots(nrows=1, ncols=2, figsize=(12, 4))\n","\n","axes[0].plot(history.epoch, history.history['accuracy'], label='train')\n","axes[0].plot(history.epoch, history.history['val_accuracy'], label='valid')\n","axes[0].set_title('Accuracy')\n","axes[0].legend()\n","axes[1].plot(history.epoch, history.history['loss'], label='train')\n","axes[1].plot(history.epoch, history.history['val_loss'], label='valid')\n","axes[1].set_title('Loss')\n","axes[1].legend()\n","\n","pyplot.tight_layout()\n","pyplot.savefig('epoch_history_dcnn.png')\n","pyplot.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#  Evaluation"],"metadata":{"id":"j8SNr9kNxF0v"}},{"cell_type":"markdown","source":["## Confusion Matrix on the test set"],"metadata":{"id":"FOQyHtwQz8gY"}},{"cell_type":"code","metadata":{"id":"AzW4cG66EDAR"},"source":["predict_x = model.predict(X_test)\n","classes_x = np.argmax(predict_x,axis= 1)\n","skplt.metrics.plot_confusion_matrix(np.argmax(y_test, axis=1), classes_x,figsize=(7,7))\n","pyplot.savefig(\"confusion_matrix_dcnn.png\")\n","\n","print(f'total wrong test predictions: {np.sum(np.argmax(y_test, axis=1) != classes_x)}\\n\\n')\n","print(classification_report(np.argmax(y_test, axis=1), classes_x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s3Mi8Vhv9MOU"},"source":["import scikitplot as skplt\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define emotion labels\n","emotion_labels = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}\n","\n","# Make predictions\n","predict_x = model.predict(X_test)\n","classes_x = np.argmax(predict_x, axis=1)\n","\n","# Plot confusion matrix\n","skplt.metrics.plot_confusion_matrix(np.argmax(y_test, axis=1), classes_x, figsize=(7, 7),\n","                                    title=\"Confusion Matrix (Emotion Labels)\", \n","                                    text_fontsize='large', cmap='Blues')\n","\n","# Replace x and y tick labels with emotion names\n","tick_marks = np.arange(len(emotion_labels))\n","plt.xticks(tick_marks, list(emotion_labels.values()), rotation=45, fontsize=12)\n","plt.yticks(tick_marks, list(emotion_labels.values()), fontsize=12)\n","\n","plt.tight_layout()\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","\n","y_true = np.argmax(y_test, axis=1)\n","cm = confusion_matrix(y_true, classes_x)\n","\n","correct_pred_per_class = np.diag(cm)\n","incorrect_pred_per_class = np.sum(cm, axis=1) - np.diag(cm)\n","\n","print(\"Correct predictions per class:\", correct_pred_per_class)\n","print(\"Incorrect predictions per class:\", incorrect_pred_per_class)\n"],"metadata":{"id":"DeOKUJ3WAFY4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.sum(correct_pred_per_class) + np.sum(incorrect_pred_per_class)"],"metadata":{"id":"zsWxFPreCfjn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mapper = {\n","    0: \"angry\",\n","    1: \"disgust\",\n","    2: \"fear\",\n","    3: \"happy\",\n","    4: \"sad\",\n","    5: \"suprise\",\n","    6: \"neutral\"\n","}\n","np.random.seed(2)\n","random_dis_imgs = np.random.choice(np.where(y_valid[:, 1]==1)[0], size=9)\n","random_fear_imgs = np.random.choice(np.where(y_valid[:, 2]==1)[0], size=9)\n","fig = pyplot.figure(1, (18, 4))\n","for i, ( disidx, fearidx ) in enumerate(zip(random_dis_imgs, random_fear_imgs)):\n","        ax = pyplot.subplot(2, 9, i+1)\n","        sample_img = X_valid[disidx,:,:,0]\n","        ax.imshow(sample_img, cmap='gray')\n","        ax.set_xticks([])\n","        ax.set_yticks([])\n","        ax.set_title(f\"true:dis, pr:{mapper[np.argmax(model.predict(sample_img.reshape(1,48,48,1))[0])]}\")\n","\n","       \n","\n","        ax = pyplot.subplot(2, 9, i+10)\n","        sample_img = X_valid[fearidx,:,:,0]\n","        ax.imshow(sample_img, cmap='gray')\n","        ax.set_xticks([])\n","        ax.set_yticks([])\n","        ax.set_title(f\"true:fear, pr:{mapper[np.argmax(model.predict(sample_img.reshape(1,48,48,1))[0])]}\")\n","\n","\n","\n","        pyplot.tight_layout()"],"metadata":{"id":"UmPfXFFYDcQz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5qdACSN5E5s0"},"execution_count":null,"outputs":[]}]}